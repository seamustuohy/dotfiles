####################################
# Scraping
####################################

alias listserv='time get_listserv'

get_site_links() {
    local SITE=${1}
    wget -w 1 --no-check-certificate --spider -r ${SITE} 2>&1 | grep '^--' | awk '{ print $3 }' | grep -v '\.\(css\|js\|png\|gif\|jpg\|JPG\)$' > urls.txt
}


# One of @janmoesen’s ProTip™s
for method in GET HEAD POST PUT DELETE TRACE OPTIONS; do
        alias "${method}"="lwp-request -m '${method}'"
done


get_all_site_links() {
    local URL="$1"
    local tmpdir=$(mktemp -dt "$(basename "$0").XXXXX")
    wget -m "${URL}" \
         --directory-prefix="${tmpdir}" \
         2>&1 \
        | grep '^--' \
        | awk '{ print $3 }' \
        | grep -v '\.\(css\|js\|png\|gif\|jpg\|JPG\)$'
    rm -fr "${tmpdir}"
}


wget_mirror_single_page() {
    # Usage: wget_mirror_single_page [Flags] [URL]
    # wget_mirror_single_page --no-check-certificate  https://site.with.broken.ssl.cert.com/example
    # Based on: https://gist.github.com/dannguyen/03a10e850656577cfb57
    # wget --adjust-extension \ # Appends .html to the local copy of whatever file youre trying to mirror:
    #      --span-hosts \ # Even though were trying to mirror a single page, that page may link to offsite assets such as images or stylesheets located on a CDN i.e. a different domain, such as s3.thissite.amazon.aws.com. However, since were only mirroring a single page, this shouldnt cause wget to crawl past the foreign hosts that are hosting these asset files.
    #      --convert-links \ # When mirroring the target webpage, we create a self-contained folder of all of its external dependencies, so that you can disconnect your computer from the Internet, open that target webpage, and see it mostly function in its own non-Internet-aware environment.
    #      --backup-converted \ # Because the original HTML are being converted via --converted-links, you might want to stash a copy of the original HTML and its original local paths/urls for posterity. This option saves those files – typically with .css and .html extensions – with an extra .orig extension.
    #      --page-requisites \ # This option causes Wget to download all the files that are necessary to properly display a given HTML page. This includes such things as inlined images, sounds, and referenced stylesheets.
    #      --timestamping \ # This checks to see if target.html.orig assuming --convert-links was used already exists locally, and if so, if it is older than the remote servers timestamp for the file. If not, then wget wont bother to re-download the page. This only works if the remote server returns a Last-Modified header.
    #      --no-directories \ # Makes a nice, flat listing of files.
    #      $@
    # set -x
    wget --adjust-extension \
         --span-hosts \
         --convert-links \
         --backup-converted \
         --page-requisites \
         --timestamping \
         --restrict-file-names=windows \
         --no-directories \
         "$@"

    # set +x
}


####################################
# Archiving
####################################

archive-get_all_archives() {
    year=$(date +"%Y")
    curl "http://timetravel.mementoweb.org/api/json/${year}/${1}" | jq
}

archive-check_is_archived() {
    URL="$1"
    archive_api="http://archive.org/wayback/available?url=${URL}"
    UA="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
    # ---------- Alternative looser status check request
    # Below commented request checks if there are any snapshots available no matter what status
    # request_string='import requests; h = {"User-Agent":"'"${UA}"'"}; url ="'"${archive_api}"'";r = requests.get(url, headers=h); print("Archives Exist") if len(r.json().get("archived_snapshots",{}).keys()) > 0 else print("No Archives for this page");'
    # -------
    # Below request checks if the closest snapshot has a status of 200
    request_string='import requests; h = {"User-Agent":"'"${UA}"'"}; url ="'"${archive_api}"'";r = requests.get(url, headers=h);print("Successful archives exist") if r.json().get("archived_snapshots",{}).get("closest",{}).get("status","401") == "200" else print("No (or failed) archives for this page")'
    python3 -c "${request_string}"
}

archive-create_archive() {
    URL="$1"
    archive_api="http://web.archive.org/save/${URL}"
    UA="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
    request_string='import requests; h = {"User-Agent":"'"${UA}"'"}; url ="'"${archive_api}"'";r = requests.get(url, headers=h); print("Archive Created") if r.status_code == 200 else print("Archive Failed: Status Code {0}".format(r.status_code));'
    python3 -c "${request_string}"
}


####################################
# OSINT
####################################

# Run `dig` and display the most useful info
function digga() {
        dig +nocmd "$1" any +multiline +noall +answer;
}

# Show all the names (CNs and SANs) listed in the SSL certificate
# for a given domain
function getcertnames() {
        if [ -z "${1}" ]; then
                echo "ERROR: No domain specified.";
                return 1;
        fi;

        local domain="${1}";
        echo "Testing ${domain}…";
        echo ""; # newline

        local tmp=$(echo -e "GET / HTTP/1.0\nEOT" \
                | openssl s_client -connect "${domain}:443" -servername "${domain}" 2>&1);

        if [[ "${tmp}" = *"-----BEGIN CERTIFICATE-----"* ]]; then
                local certText=$(echo "${tmp}" \
                        | openssl x509 -text -certopt "no_aux, no_header, no_issuer, no_pubkey, \
                        no_serial, no_sigdump, no_signame, no_validity, no_version");
                echo "Common Name:";
                echo ""; # newline
                echo "${certText}" | grep "Subject:" | sed -e "s/^.*CN=//" | sed -e "s/\/emailAddress=.*//";
                echo ""; # newline
                echo "Subject Alternative Name(s):";
                echo ""; # newline
                echo "${certText}" | grep -A 1 "Subject Alternative Name:" \
                        | sed -e "2s/DNS://g" -e "s/ //g" | tr "," "\n" | tail -n +2;
                return 0;
        else
                echo "ERROR: Certificate not found.";
                return 1;
        fi;
}

############################################
## Indicator Capture
############################################




# `-u` for unique
# -t \: for colon delimited
# -k1,1 for key field 1
# alias unique_http_or_https_transport="grep 'http' | sort -u -t\/ -k4,4"


http_unique() {
    grep -Eo "http[s]://[ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\._~:/\?#\&\(\)\*+\,\;=@ -]+" "${1}" \
        | sort -u -t\/ -k4,4
        # `-u` for unique
        # -t \: for colon delimited
        # -k1,1 for key field 1
}


search_dir_for_urls() {
    grep -r --only-matching --perl-regexp "http(s?):\/\/[^ \"\(\)\<\>]*" "${1}"
}



############################################
## Email
############################################

msg2eml() {
    # Convert a .msg email to a .eml email
    # sudo apt-get install libemail-outlook-message-perl libemail-localdelivery-perl
    local MSG_FILE="${1}"
    local EML_FILE="${2}"
    ~/tools/msg2eml/msgconvert.pl --verbose --mbox "${EML_FILE}" "${MSG_FILE}"
}

read_email() {
    # Take a .eml email and open it up in mutt
    # sudo apt-get install mutt procmail
    local curdir=$(pwd)
    local tmpdir
    tmpdir=$(mktemp -dt "$(basename "$0").XXXXX")
    local eml_name
    eml_name=$(basename "$0")
    cp -f "$1" "${tmpdir}/${eml_name}"
    cd "${tmpdir}"
    formail -b < "${eml_name}" > "${eml_name}.mbox"
    neomutt -F '' -f "${eml_name}.mbox"
    cd "${curdir}"
    echo "Files Can be found in ${tmpdir}"
}


# ===== VIPER ALIASES =====
# For use in viper docker container.

alias oledump="python2 /tools/oledump/oledump.py --plugindir /tools/oledump"

#  --pluginoptions=-k
alias msgdump="python2 /tools/oledump/oledump.py --plugindir /tools/oledump -q -p plugin_msg"




############################################
## Android
############################################

apk_certificate() {
    keytool -list -printcert -jarfile "${1}"
}

apk_verify_signature() {
    jarsigner -verify -verbose:summary -certs "${1}"
}


############################################
## Viper Helpers
############################################

get_shortened_from_viper_line() {
    # For reading shortened links copied from a pdf using viper `strings -N`
    local viper_line="$1"
    local url=$(echo  "${viper_line}" | sed 's/^ - .*\/URI (\(.*\)).*$/\1/')
    local location=$(curl -sI "${url}" | grep Location | cut -d : -f 2-)
    printf "%s,%s\n" "${url}" "${location}"
}

read_viper_url_file() {
    # $1 == url file
    # $2 == output file to write links to
    if [ $# -eq 2 ]; then
        echo "OVERWRITING FILE: $2"
        printf "Continue? "
        read
        echo "shortened_link,destination_url" > "$2"
    fi
    local filelen=$(wc -l "$1")
    iterator=0
    while IFS='' read -r viper_line || [[ -n "$viper_line" ]]; do
        local printable=$(get_shortened_from_viper_line "${viper_line}")
        if [ $# -eq 2 ]; then
            iterator=$(("$iterator" + 1))
            printf "Getting %s of ${filelen}\r" "$iterator"
            printf "%s\n" "${printable}" >> "$2"
        else
            printf "%s\n" "${printable}"
        fi
    done < "$1"
    printf "\n"
}

get_thug_from_viper_url_file() {
    # $1 == file from read_viper_url_file
    # $2 == output file to write location of thug data
    if [ $# -eq 2 ]; then
        echo "OVERWRITING FILE: $2"
        printf "Continue? "
        read
        echo "shortened_link,thug_data" > "$2"
    else
        printf "Please provide an output file \n"
        return 1
    fi
    # set -x
    local filelen=$(wc -l "$1" | cut -d ' ' -f1 )
    filelen=$(("$filelen" - 1))
    iterator=0
    # local url_list=$(sed 1,1d "$1")
    local url_list=$(sed 1,1d "$1" | cut -d , -f2 | sed '/^\s*$/d' | sort | uniq)
    # echo "$url_list"
    while IFS='' read -r url_line || [[ -n "$url_line" ]]; do
        iterator=$(("$iterator" + 1))
        #printf "%s,%s\n"
        local trimmed_url=$(echo "${url_line}" | tr -d '[:space:]')
        printf "Getting %s:\n %s of %s\n" "${trimmed_url}" "$iterator" "${filelen}"
        local output_path=$(thug_unassisted "${trimmed_url}" | grep "Thug Output Path" | sed 's/.*\(Thug Output Path: \)\(\/.*\).*$/\2/')
        printf "%s,%s\n" "${trimmed_url}" "${output_path}" >> "$2"
        #printf "%s,%s\n" "${url_line}" "${output_path}"
        # local printable=$(get_shortened_from_viper_line "${viper_line}")
    done <<< "$url_list"
    # set +x
}

get_bitly_hashes() {
    # ONLY gets bit.ly hashes (for harpoon)
    # $1 == file from read_viper_url_file
    # $2 == output file to write location of thug daat
    cat "$1" | cut -d, -f 1| grep "bit\.ly" | sed 's/http[s]*:\/\/bit\.ly\/\(.*\)/\1/' > "$2"
}



############################################
## Extraction From Files
############################################

extract_images() {
    local filename="$1"
    exiftool -a -b -ee -embeddedimage -W Image_%.3g3.%s "${filename}"
    # exiftool -a -b -ee -W Embeded_%.3g3.%s "${filename}"
    # Extract embedded JPG and JP2 images from a PDF file.  The output images will have file names like
    # "Image_#.jpg" or "Image_#.jp2", where "#" is the ExifTool family 3 embedded document number for the
    # image.
    # Use -g3 or -G3 to identify the originating document for extracted information.

}

alias exiftool_extract_images=extract_images

extract_ALL_metadata() {
    local filename="$1"
    exiftool -a -ee -u -g1 -api RequestAll=3 "${filename}"
# Print all meta information in an image, including duplicate and unknown tags, sorted by group (for family 1). For performance reasons, this command may not extract all available metadata. (Metadata in embedded documents, metadata extracted by external utilities, and metadata requiring excessive processing time may not be extracted). Add -ee and -api RequestAll=3 to the command to extract absolutely everything available.

}

alias exiftool_show_ALL=extract_ALL_metadata


############################################
## Forensics
############################################


# Extract the binary of running process (Even if the original binary is deleted)
get_process_binary_by_pid() {
    local PID="${1}"
    cat "/proc/${1}PID/exe"
}

# Copy the binary of running process (Even if the original binary is deleted) somewhere
# This is great for when you REALLY mess up a system and "irreversibly" delete a critical script that is still being used by an active process
cp_process_binary_by_pid() {
    local PID="${1}"
    local OUTPUT="${2}"
    cp "/proc/${1}PID/exe" "${OUTPUT}"
}


# Untested
clamscan_extract_autoit_scripts() {
    filename="$1"

    tempfile = $(clamscan ---debug --verbose --leave-temps "${filename}" 2>&1 | grep autoit | grep "script extracted to" | cut -d/ -f 2-)
    cat "/${tempfile}"
}



####################################
# Encode / Decode
####################################

# URL-encode strings

alias encode_url_percent='python -c "import sys, urllib as ul; print ul.quote_plus(sys.argv[1]);"'

# Url-decode strings
decode_percent() {
    echo "${1}" | python3 -c 'import urllib.parse, sys; s=sys.stdin.read(); print(urllib.parse.unquote(s))'
}
alias unescape_percent=decode_percent
alias decode_percent_pipe='sed "s@+@ @g;s@%@\\\\x@g" | xargs -0 printf "%b"'
alias unescape_percent_pipe=decode_percent_pipe

# UTF-8-encode a string of Unicode symbols
encode_x() {
        local args
        mapfile -t args < <(printf "%s" "$*" | xxd -p -c1 -u)
        printf "\\\\x%s" "${args[@]}"
        # print a newline unless we’re piping the output to another program
        if [ -t 1 ]; then
                echo ""; # newline
        fi
}

# Decode \x{ABCD}-style Unicode escape sequences
decode_x() {
        perl -e "binmode(STDOUT, ':utf8'); print \"$*\""
        # print a newline unless we’re piping the output to another program
        if [ -t 1 ]; then
                echo ""; # newline
        fi
}
alias unescape_x=decode_x

## Decoding ampersand hash strings
# https://web.archive.org/web/20200824165514/https://deskspace.com/html_special_characte.html
decode_ampersand() {
    echo "${1}" | python3 -c 'import html, sys; s=sys.stdin.read(); print(html.unescape(s))'
}
alias unescape_ampersand=decode_ampersand
decode_ampersand_hash() {
    # echo "${1}" | python3 -c 'import sys; s=sys.stdin.read(); print("".join([chr(int(u.strip(";\n, "))) for u in s.split("&#") if u]))'
    echo "${1}" | python3 -c 'import html, sys; s=sys.stdin.read(); print(html.unescape(s))'
}
alias unescape_ampersand_hash=decode_ampersand_hash

decode_email_header() {
    echo "${1}" | python3 -c "from email.header import decode_header; import sys; h=sys.stdin.read(); print(decode_header(h))"
}
alias unescape_email_header=decode_email_header
