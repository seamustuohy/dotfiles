####################################
# Get Media from Online
####################################

get_song_from_youtube() {
      local SITE=${1}
      local LOCATION=$(pwd)
      cd ~/Music
      youtube-dl --audio-format mp3 -x ${SITE}
      cd ${LOCATION}
  }

alias getsong=get_song_from_youtube


# Grab a compressed file from online and uncompress it
curltar() {
        case $1 in
                *.tar.bz2) \curl -kL $1 | tar xvjf - ;;
                *.tar.gz) \curl -kL $1 | tar xvzf - ;;
                *.bz2) \curl -kL $1 | bunzip2 - ;;
                *.rar) \curl -kL $1 | unrar x - ;;
                *.gz) \curl -kL $1 | gunzip - ;;
                *.tar) \curl -kL $1 | tar xvf - ;;
                *.tbz2) \curl -kL $1 | tar xvjf - ;;
                *.tgz) \curl -kL $1 | tar xvzf - ;;
                *.zip) \curl -kL $1 | unzip - ;;
                *.Z) \curl -kL $1 | uncompress - ;;
                *.7z) \curl -kL $1 | 7z x - ;;
                *) \curl -kLO $1
        esac
}

curl_w_resume() {
    # From the following one liner
    # export ec=18; while [ $ec -eq 18 ]; do /usr/bin/curl -O -C - "http://..........zip"; export ec=$?; done
    local url="${1}"
    local ec=18;
    while [ $ec -eq 18 ]; do
        curl -O -C - "${url}";
        ec=$?;
    done
}



# Evidence Collection

get_evidence_wget_from_url() {
    local url="$1"
    local urlid=$(echo "$url" | tr -cd '[:alnum:]._-')
    local urlid="${urlid:0:10}-$(uuidgen)"
    mkdir "${urlid}"
    cd "${urlid}"
    wget_mirror_single_page \
             --no-check-certificate \
             -U "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36" \
             "${url}"
    cd ..
    echo "Stored at ${urlid}"
}

get_evidence_youtube_from_url() {
    local url="$1"
    local urlid=$(echo "$url" | tr -cd '[:alnum:]._-')
    local urlid="${urlid:0:10}-$(uuidgen)"
    mkdir "${urlid}"
    cd "${urlid}"
    get_youtube_evidence --no-check-certificate "${url}"
    cd ..
    echo "Stored at ${urlid}"
}

get_youtube_evidence() {
    # Downloads Youtube videos as evidence
    youtube-dl -o '%(id)s-%(title)s-%(uploader)s-%(upload_date)s.%(ext)s' \
               --write-description \
               --write-annotations \
               --write-all-thumbnails \
               --all-subs  --write-sub --write-auto-sub \
               --write-info-json \
               $@
}

####################################
# Read Media
####################################

decode_qr() {
    # decode a QR code from an image
    # sudo apt-get install zbar-tools
    zbarimg "${1}"
}


markdown_quick_view() {
    pandoc -s -f markdown -t man "${1}" | man -l -
}


####################################
# Convert and Format Media
####################################

# Create a data URL from a file
function dataurl_create() {
        local mimeType=$(file -b --mime-type "$1");
        if [[ $mimeType == text/* ]]; then
                mimeType="${mimeType};charset=utf-8";
        fi
        echo "data:${mimeType};base64,$(openssl base64 -in "$1" | tr -d '\n')";
}



rip_dvd() {
    local VOB_PATH="$1" # VTS_01_1.VOB
    local AVI_PATH="$2" # ~/video_01_1
    # -f avi : force output format
    # -c:v libx264 : encode with the h264 codec to create small, but quality files
    # -g 300 : GOP size is 300 which means one intra frame every 10 seconds for 29.97fps input video
    avconv -i "${VOB_PATH}" -f avi -c:v libx264 -g 300 -bf 2 "${AVI_PATH}".avi

    # Note for mounting dvd's
    # sudo mount -o loop /dev/cdrom /media/cdrom/
}

alias vob_to_avi="rip_dvd"


# Latex
inkscape2latex() {
    inkscape -D -z --file=${1}.svg --export-pdf=${1}.pdf --export-latex
}


# Whiteboard Cleaning
white-board-cleaner () {
    convert "$1" -morphology Convolve DoG:15,100,0 -negate -normalize -blur 0x1 -channel RBG -level 60%,91%,0.1 "$2" ;
}


# This is just a stub that can be modified to work better
# with different photos
annotate() {
    # annotate [image_name] "[annotation]"
    # produces text_[image_name] file
    # ONLY use this in the same directory as the image.

    local image_name=$1
    local annotation=$2

    # North, South, East, West
    # NorthEast, SouthWest, Etc.
    local gravity="East"
    # Font Selection
    # See all with following command
    # convert -list font | grep "Font:"
    local font="Droid-Sans"
    local pointsize=25
    # Fold the text at a certain char
    # change this to make the text wrap
    local foldchar=15
    # This makes it only fold on spaces
    local spacefold="-s"
    # This would make it not fold.
    # local spacefold=""

    convert -gravity $gravity \
            -weight 800 \
            -font $font -pointsize $pointsize \
            -annotate +20+20 "$(fold -w ${foldchar} ${spacefold} <<< "${annotation}")" \
            "$image_name" text_"$image_name"
}


convert_fill_background_with_white() {
    convert "${1}" \
    -fill White \
    -fuzz 25% \
    -draw 'color -1,-1 replace' \
    -trim \
    -bordercolor White \
    -border 10x10 \
    ${filename/.*/_cleaned}.${filename/*./}
}


clean_yellowed_book_page() {
    filename="${1}"
    color=$(~/dotfiles/bin/get_bright_colors.sh "${filename}" | sort | tail -1 | cut -d ' ' -f2)
    convert_replace_color_with_white ${filename} $color
}

alias convert_clean_yellowed_book_page=clean_yellowed_book_page
convert_replace_color_with_white() {
    filename="${1}"
    hex="${2}"
    convert "${filename}" -fuzz 20% -fill "#ffffff" -opaque "#${hex}" ${filename/.*/_cleaned}.${filename/*./}
}

function get_color_histogram() {
    for i in $(convert ${1} -colors 16 -depth 8 -format "%c" -alpha off histogram:info: 2>@1 \
                   | cut -d '#' -f 2 \
                   | cut -d ' ' -f 1); do \
        echo  -e "\e[48;5;$(fromhex $i)m     \e[0m" \
        ; done
}

pdf_remove_password() {
  # Requires xpdf-utils
  local filename="$1"
  local password="$2"
  # pdftk input.pdf output output.pdf user_pw YOURPASSWORD-HERE
  # pdftk input.pdf output output.pdf user_pw YOURPASSWORD-HERE owner_pw YOURPASSWORD-HERE
  # pdftk input.pdf output output.pdf input_pw YOURPASSWORD-HERE
  pdftops -upw "${password}" "${filename}"
  ps2pdf "${filename/%.pdf/.ps}" "${filename/%.pdf/_no_password.pdf}"
}

pdf_remove_feature_protections() {
    local filename="$1"
    ps2pdf "${filename}" "${filename%.pdf}-unlocked.pdf"
}

pdf_extract_images() {
  # set -x
  local filename="$1"
  # Kill leading whitespace (found in find) and replace all non ASCII with underscores
  local outdir=$(echo "$1" | sed -e 's/^[[:space:]]*//' | sed -e 's/[^a-zA-Z0-9]/_/g')
  local filedir=$(dirname "${filename}")
  local tmpdir=$(mktemp -dt "$(basename "EXTRACT_PDF_IMAGES").XXXXX")
  pdfimages -all "${filename}" "${tmpdir}/image"
  mkdir "${filedir}/${outdir}"
  mv ${tmpdir}/image-* "${filedir}/${outdir}/."
  # set +x
}




function mov2gif() {
    name=`basename -s .mov $1`
    echo "$1 => $name.gif"
    ffmpeg -i "$1" -vf scale=980:-1 -pix_fmt rgb24 -r 10 -f gif - | gifsicle --optimize=2 --delay=6 > "${name}.gif"
}


####################################
# Bulk Download Media from Webpage
####################################


scrape_files_from_url() {
    local url="$1"
    local filetype="$2"
    local is_verify_off="$3"
    if [[ "${is_verify_off}" == "NO_VERIFY" ]]; then
        verify_flag="--no-check-certificate"
    else
        verify_flag="-i"
    fi
    IFS=$'\n'
    for item in $(python3 ~/dotfiles/bin/scrape_links_with_text_by_filetype.py -u "$url" -f "$filetype" "$verify_flag"); do
        local fileurl=$(echo "$item" | sed 's/\(.*\)::::\(.*\)/\1/g')
        local filename=$(echo "$item" | sed 's/\(.*\)::::\(.*\)/\2/g' | sed 's/[[:punct:]]//g' | sed 's/[[:space:]]/_/g')
        local extension=$(echo "$item" | sed 's/\(.*\)::::\(.*\)/\1/g'| rev | cut -d. -f1 | rev)
        #echo "$item"
        #echo $fileurl
        #echo "${filename}.${extension}"
        wget "$verify_flag" -O "${filename}.${extension}" "${fileurl}"
     done
}

alias download_files_from_url="scrape_files_from_url"

scrape_files_from_url_no_rename() {
    local url="$1"
    local filetype="$2"
    local is_verify_off="$3"
    if [[ "${is_verify_off}" == "NO_VERIFY" ]]; then
        verify_flag="--no-check-certificate"
    else
        verify_flag=""
    fi
    wget -r -np -nd -l 1 -A "$filetype" "$verify_flag" "${url}"
}


download_open_directory_files_only_continuable() {
    #set -x;
    regex='^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\?([^#]*))?(#(.*))?'

    if [[ $1 =~ $regex ]]; then
        dirs=${BASH_REMATCH[5]}
        trimmed="${dirs:1:${#dirs}-2}"
        slashes="${trimmed//[^\/]}"
        count="${#slashes}"
        wget --no-check-certificate -q --show-progress --timestamping -c -N -r -np -nH --cut-dirs=${count} -R "index.html*" "$1/"
    else
        echo "URL not understood."
    fi
    #set +x;
}

alias wget_download_open_directory=wget_download_open_directory

wget_download_open_directory_full_continuable() {
    wget -q --show-progress -r -np -nH -c -N "$1/"
}

unprotect_docx() {
    echo "This will only remove restrictions to editing. It will not un-encrypt a docx file."
    doc_path="${1}"
    tmpdir=$(mktemp -d)
    curdir=$(pwd)
    unzip "${doc_path}" -d "${tmpdir}"
    find "${tmpdir}"/. -name "*.xml" -exec sed -i 's/<w:documentProtection[^>]*>//' {} +
    echo "TEMPDIR IS ${tmpdir}"
    cd  "${tmpdir}"
    find . -type f | xargs zip "${curdir}/${1%.docx}-unprotected.docx"
    cd "${curdir}"
    rm -fr "${tmpdir}"
}

unprotect_xlsx() {
    echo "This will only remove sheet and workbook protections. It will not un-encrypt a xlsx file."
    doc_path="${1}"
    tmpdir=$(mktemp -d)
    curdir=$(pwd)
    unzip "${doc_path}" -d "${tmpdir}"
    find "${tmpdir}"/. -name "*.xml" -exec sed -i 's/<sheetProtection[^>]*>//' {} +
    find "${tmpdir}"/. -name "*.xml" -exec sed -i 's/<workbookProtection[^>]*>//' {} +
    echo "TEMPDIR IS ${tmpdir}"
    cd  "${tmpdir}"
    find . -type f | xargs zip "${curdir}/${1%.xlsx}-unprotected.xlsx"
    cd "${curdir}"
    rm -fr "${tmpdir}"
}




alias find_duplicate_images=findimagedupes
